\documentclass[12pt]{book}

\usepackage[dvips,letterpaper,margin=0.75in,bottom=0.5in]{geometry}
\usepackage{cite}
\usepackage{slashed}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{braket}
\begin{document}

\newcommand{\ihbar}{\ensuremath{i \hbar}}
\newcommand{\Pss}{\ensuremath{\Psi^*}}
\newcommand{\dPsidt}{\ensuremath{ \frac{\partial \Psi}{\partial t} }}
\newcommand{\dPsidx}{\ensuremath{ \frac{\partial \Psi}{\partial x} }}
\newcommand{\ddPsidx}{\ensuremath{ \frac{\partial^2 \Psi}{\partial x^2} }}
\newcommand{\dPssdt}{\ensuremath{ \frac{\partial \Psi^*}{\partial t} }}
\newcommand{\dPssdx}{\ensuremath{ \frac{\partial \Psi^*}{\partial x} }}
\newcommand{\ddPssdx}{\ensuremath{ \frac{\partial^2 \Psi^*}{\partial x^2} }}

\newcommand{\dphidt}{\ensuremath{ \frac{d \phi}{dt} }}
\newcommand{\dpsidx}{\ensuremath{ \frac{d \psi}{dx} }}
\newcommand{\ddpsidx}{\ensuremath{ \frac{d^2 \psi}{dx^2} }}


\title{PHY 115A \\ Lecture Notes 3: \\ 
Formalism \\
(Griffith's Chapter 3)}
\author{Michael Mulhearn}

\maketitle

\setcounter{chapter}{2}
\chapter{Formalism}

\section{Matter Waves}

Early in the development of quantum mechanics it became clear that light was quantized as photons with discrete energy:
$$E=h\nu$$
but for a massless particle, we know that:
$$E = pc$$
and:
$$pc = h\nu$$ 
so that:
$$\frac{h}{p} = \frac{\nu}{c} = \lambda$$
where $\lambda$ is the wavelength of the light.  De Broglie made the hypothesis that matter was also described by a wave with frequency:
$$\nu = \frac{E}{h}$$
and wavelength:
$$\lambda = \frac{h}{p}$$
We are using the reduced Planck's constant:
$$\hbar = \frac{h}{2\pi}$$
so we write these relations equivalently as:
$$E = h\nu = \hbar (2\pi \nu) = \hbar \omega$$
and:
$$p = \frac{h}{\lambda} = \hbar \frac{2\pi}{\lambda} = \hbar k$$
Recall that in general a right traveling wave has the format:
$$f(k\,x-\omega \, t)$$
for $k>0$ and $\omega>0$.  So we can imagine several different functional forms for de Broglie's ``matter waves" traveling left:
$$\Psi_1(x,t) = \cos(kx - \omega t)$$
or:
$$\Psi_2(x,t) = \sin(kx - \omega t)$$
or
$$\Psi_3(x,t) = \exp(ikx - i\omega t)$$
where we don't care about the amplitude of the function yet.  But take care to note that:
$$g(-k\,x+\omega \, t) = g(-(k\,x-\omega \, t))$$
is also a right traveling wave, because it can be written as:
$$f(k\,x-\omega \, t) \equiv g(-(k\,x-\omega \, t))$$
But since:
$$\cos(-k\,x+\omega \, t) = \cos(k\,x-\omega \, t)$$
we don't need to consider that case.  And since:
$$\sin(-k\,x+\omega \, t) = -\sin(k\,x-\omega \, t)$$
and we don't care about the amplitude of the function yet, we can ignore that as well.  That only leaves one additional possibility to consider:
$$\Psi_4(x,t) = \exp(-ikx + i\omega t)$$

It seems like we could use any of these four options, but let's see what happens when we try to construct a standing wave by adding am equal mixture of right traveling and left traveling waves.  To switch to a left traveling wave, we just put:
$$k \to -k$$
Our standing wave using $\Psi_1$ is:
$$\Psi(x) = \cos(k\,x-\omega \, t) + \cos(-k\,x-\omega \, t)$$
but using:
$$\cos(\alpha + \beta) = \cos(\alpha)\cos(\beta) - \sin(\alpha)\sin(\beta)$$
we conclude:
$$\Psi(x) = 2 \cos(k\,x) \cos(\omega \, t)$$
which is an utter catastrophe, since the wave function vanishes everywhere whenever:
$$\omega t = \frac{n \pi}{2} \hspace{2cm} n=1,3,5$$
This would describe a particle that disappears and reappears from existence... not the theory we are trying to build!  So we discard option $\Psi_1$. Likewise for $\Psi_2$:
$$\Psi(x) = \sin(k\,x-\omega \, t) + \sin(-k\,x-\omega \, t)$$
but using:
$$\sin(\alpha + \beta) = \sin(\alpha)\cos(\beta) + \cos(\alpha)\sin(\beta)$$
we conclude:
$$\Psi(x) = -2\cos(k\,x) \sin(\omega \, t)$$
which is another catastrophe, since the wave function vanishes everywhere whenever:
$$\omega t = n \pi \hspace{2cm} n=1,2,3,4,5$$
But there is no such problem with $\Psi_3$ where:
$$\Psi(x) = \exp(ik\,x-i\omega \, t) + \exp(-ik\,x-i\omega \, t) = 2 \cos(kx) e^{\displaystyle -i\omega t}$$
this vanishes at specific locations in space, where the particle will never be found, but this is exactly what we would expect for a standing wave.  The only time-dependent factor 
$$e^{\displaystyle -i \omega t}$$
is never zero at any point in time.  Similarly, for $\Psi_4$:
$$\Psi(x) = \exp(-ik\,x+i\omega \, t) + \exp(ik\,x+i\omega \, t) = 2 \cos(kx) e^{\displaystyle i\omega t}$$
So it seems at this point that both $\Psi_3$ and $\Psi_4$ are valid options for a right-traveling matter waves, but if they are both valid, then we should be able to superimpose them:
$$\Psi_3(x,t) + \Psi_4(x,t) = \exp(ik\,x-i\omega \, t) + \exp(-ik\,x+i\omega \, t)
= 2 \cos(k\,x-\omega\,t)$$
which we already saw was a problematic right traveling wave.  So we conclude that we can have $\Psi_3$ or $\Psi_4$, but {\bf not both}.  We choose the one that has $k>0$ for a right traveling wave, so that our right traveling matter wave is finally uniquely identified as:
\begin{equation}
\Psi_{\rm k}(x,t) \equiv \exp(ik\,x-i \omega \, t)
\end{equation}
with where:
$$p=\hbar k, \hspace{1cm} {\rm and} \hspace{1cm} E = \hbar \omega.$$
For the free particle, we also have:
\begin{equation}
E = \frac{p^2}{2m} = \frac{\hbar^2 k^2}{2m}
\end{equation}
and so:
\begin{equation}
\omega(k) = \frac{\hbar k^2}{2m} 
\end{equation}
The free particles have phase velocity:
$$v_{\rm phase} = \frac{\omega}{k} = \frac{\hbar k}{2m}$$
and group velocity:
$$v_{\rm group} = \frac{d\omega}{dk} = \frac{\hbar k}{m}$$
which is the classical velocity of the particle.  In chapter 2, we showed that this phase velocity is the velocity of a wave packet consisting of waves with wave numbers close to $k$.

 \section{From Matter Waves to the Schr\"odinger Equation}

With the matter waves in hand, we now will try to identify the wave equation that predicts matter waves for a free particle ($V(x)=0$), in the hope that we can generalize that equation in other contexts ($V(x) \neq 0$).

The first thing we will try to do is determine the momentum of the matter wave.  Now, knowing that:
$$\Psi_k(x,t) = \exp(ik\,x-i \omega \, t)$$
we can of course simply read off the wave number $k$ and determine the momentum $p=\hbar k$.  But we cannot {\it generalize} that approach to other wave functions $\Psi(x,t)$.  In the general case, we will only know $\Psi(x,t)$, so we can only do things involving $x$ and $t$.  Here's something:
$$-i\hbar \frac{\partial}{\partial x} \Psi_k(x,t) = (-i \hbar) (ik) \Psi_k(x,t) = \hbar k \Psi_k(x,t) = p \Psi_k(x,t)$$
so we define the momentum operator:
\begin{equation}
\hat{p} \equiv -i \hbar \frac{\partial}{\partial x}
\end{equation}
and note that for our matter wave for free particles:
\begin{equation}
\hat{p} \, \Psi_k(x,t) = p \, \Psi_k(x,t)
\end{equation}
We say that the free particle solutions $\Psi_k(x,t)$ is an eigenstate of the momentum operator $\hat{p}$.  Note that this is true only for eigenstates.  For a general state with wave function $\Psi(x,t)$ generally:
$$\hat{p} \, \Psi(x,t) \neq p \, \Psi(x,t)$$
Note also that $\hat{p}$ and $p$ are two very different things.  Whereas $p$ is just a number, $\hat{p}$ is a complicated beast: an operator that when given a wave function returns a (possibly different) wave function.

What about the position operator $\hat{x}$?  Since we have access to $x$ and $t$, for any wave function $\Psi(x,t)$, we need only write:
\begin{equation}
\hat{x} = x
\end{equation}
and so for {\em any wave function} $\Psi(x,t)$
$$\hat{x} \, \Psi(x,t) = x \, \Psi(x,t)$$
which looks like any $\Psi(x,t)$ is also an eigenstate of $\hat{x}$, but that is not so, because $x$ is not just a number here, and 
$$x \, \Psi(x,t)$$
is a new wave function that is distinct from $\Psi(x,t)$.

Next we will try to find a recipe for determining the energy $E=\hbar \omega$ only through $\Psi(x,t)$.  We can try something similar:
$$i\hbar \frac{\partial}{\partial t} \, \Psi_k(x,t) = (i\hbar)(-i\omega) \, \Psi_k(x,t) = \hbar \omega \, \Psi_k(x,t) = E \, \Psi_k(x,t)$$
We also know that for the free particle:
$$E = \frac{p^2}{2m}$$
So let's find an operator $\hat{H}$ such that:
$$\hat{H} \Psi_k(x,t) = \frac{p^2}{2m} \Psi_k(x,t)$$
for our free particles.  Working with our definition of the momentum operator:
$$\hat{H} \, \Psi_k(x,t) = \frac{p^2}{2m} \Psi_k(x,t) = \frac{p}{2m}(p\,\Psi_k(x,t)) = \frac{p}{2m}\;
\left(-i \hbar \frac{\partial}{\partial x}\,\Psi_k(x,t)\right)$$
but $p$ here is just a number (unlike $\hat{p}$ or $\partial/\partial x$) so we are free to move it right up against $\Psi_k(x,t)$ and continue on:
$$\hat{H} \, \Psi_k(x,t) = \frac{-i\hbar}{2m}\;\frac{\partial}{\partial x}\;(p\,\Psi_k(x,t))
= \frac{-i\hbar}{2m}\;\frac{\partial}{\partial x}\;
\left(-i \hbar \frac{\partial}{\partial x}\,\Psi_k(x,t)\right)=-\frac{\hbar^2}{2m}\,\frac{\partial^2}{\partial x^2}\,\Psi_k(x,t)$$
from which we read off the operator $\hat{H}$ for the free particle:
$$\hat{H} = -\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2} = \frac{\hat{p}^2}{2m}$$
this is the Hamiltonian operator (for the free particle) which does not have any potential energy.  For a general potential, we need to find an operator $\hat{V}$ and the total energy will be obtained by:
$$\hat{H} = \frac{\hat{p}^2}{2m} + \hat{V}$$
But the potentials we will encounter will depend only on $x$ and $t$, so the operator $\hat{V}$ is trivial:
$$\hat{V} = V(\hat{x},t) = V(x,t)$$
So for the free particle with $V(x,t)=0$ we have shown that:
$$i\hbar \frac{\partial}{\partial t} \Psi_k(x,t) = \hat{H} \, \Psi_k(x,t)$$
but our assumption is that when $V(x,t) \neq 0$ we can determine the possible states as solutions to the equation:
\begin{equation}
i\hbar \frac{\partial}{\partial t} \Psi(x,t) = \hat{H} \, \Psi(x,t)
\end{equation}
or equivalently
\begin{equation}
i\hbar \frac{\partial}{\partial t} \Psi(x,t) = -\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2}\, \Psi(x,t) + V(x,t) \Psi(x,t)
\end{equation}
So we have plausibly deduced the Schr\"odinger Equation from the deBroglie hypothesis via the Matter Waves.  

\section{Hilbert Space}

In quantum mechanics, the state of a system is represented by a wave function, $\Psi(x,t)$ and we interpret $|\Psi(x,t)|^2$ as the probability density for measuring the particle at position $x$ at time $t$.  For this interpretation to work, we must have:
$$\int_{-\infty}^{+\infty} |\Psi(x,t)|^2 dx = 1$$
But for a function $f(x)$ to be a candidate for a wave function we need only statisfy the weaker condition:
$$\int_{-\infty}^{+\infty} |f(x)|^2 dx = C < \infty$$
for then we can always just multiple $f(x)$ by $\sqrt{1/C}$ to obtain a properly normalized wave function.  We call such a function a square-integrable function.

We saw in the Fourier Series appendix that periodic functions satisfy the axiomatic properties of an abstract vector space.  What about square-integrable functions?  They are closed under addition as a consequence of the Schwarz inequality:
\begin{equation}
\left|\int_a^b \, f(x)^* g(x) \, dx\right| \leq \sqrt{\int_a^b |f(x)|^2 \, dx \; \int_a^b |g(x)|^2 \, dx}
\end{equation}
with which you should be able to show that if $f(x)$ and $g(x)$ are square integrable, then:
$$\int_{-\infty}^{+\infty} f(x) + g(x) \; dx < \, \infty$$
The other properties are of an abstract vector space are easy to establish for square integrable functions, as they follow immediately from the familiar algebraic properties of functions. 

We define an inner product for this vector space as:
$$\braket{f|g} \equiv \int_a^b \, f(x)^* g(x) \, dx$$
Here again the Schwarz inequality shows that for vectors $f$ and $g$ the inner product is finite.  Most of the properties of an inner product again follow directly from familiar algebraic properties.  One subtlety arises when showing that:
$$\braket{f|f}=0$$
if and only if:
$$f(x)=0$$
To see the challenge, consider the (non-continuous) square-integrable function:
$$f(x)=
\begin{cases}
1 & x=\pi \\
0 & {\rm otherwise} \\
\end{cases}
$$
which has $\braket{f|f}=0$ but is not $f(x)=0$.  The solution is to define two functions $f$ and $g$ as equivalent (the same vector) whenever:
$$\int_{-\infty}^{+\infty} |f(x) - g(x)|^2 \; dx = 0 $$
So now clearly if:
$$\braket{f|f} = 0$$
then:
$$\int_{-\infty}^{+\infty} |f(x)|^2 \; dx = 0 $$
and so too:
$$\int_{-\infty}^{+\infty} |f(x) - 0|^2 \; dx = 0 $$
so that $f(x)$ is equivalent to $g(x)=0$.

Last we turn to the property of completeness:  any sequence of functions that gets closer and closer together will converge to a vector in the inner-product space.
That is, if a sequence of functions $\{f_n(x)\}$ has the property that:
$$\int_{-\infty}^{+\infty} |f_n(x) - f_m(x)|^2 \; dx \to 0, \hspace{1cm} {\rm as} \hspace{1cm} m,n \to \infty$$
then:
$$f_n(x) \to f(x), \hspace{1cm} {\rm as} \hspace{1cm} n \to \infty$$
for some vector $f(x)$ in the vector space.

And with this property, we run into serious trouble, for we already know that $\delta(x)$ is the limit of a sequence of taller and narrower square-integrable functions, but it isn't itself square integrable.  Nonetheless, the fact is that one can build a complete inner product space that contains the $\delta$-function and the limits of any sequence of square-integrable functions.  But to do so requires complicating {\bf everything} in order to properly handle vectors like the $\delta$ function.  Physicists make a different choice: we keep to our much simple definitions that apply perfectly well to the square integrable wave functions we encounter, and simply accept that we will also occasionally encounter the delta function posing as if it were an actual function.  We find that, inevitably, we integrate this delta function away before obtaining any observable result.

The mathematical term for a complete inner-product space is a Hilbert Space.  Apart from side-stepped the issue of completeness, we have shown that the square-integrable functions form a Hilbert Space.  
For the state described by wave function:
$$\Psi(x,t)$$
we will write this as a vector in Hilbert space like this:
$$\ket{\Psi}$$.

\section{Observables}

We've already seen that observable quantities are associated with operators, such as $\hat{x}$ and $\hat{p}$.  In the context of Hilbert Space, an operator $\hat{O}$ is function that returns a (possibly new) vector for any given vector:
$$\hat{O}\ket{\Psi} = \ket{\Phi}$$ 
or we might choose to write that equivalently as:
$$\hat{O}\ket{\Psi} = \ket{\hat{O} \Psi}$$ 
where the LHS is an operator acting on the vector $\Psi$ whereas the RHS is the vector obtained by acting on $\Psi$ with the operator $\hat{O}$.  We will only consider linear operators with the defining properties:
$$\hat{O}(\ket{\Psi}+\ket{\Phi}) = \hat{O}\ket{\Psi}+\hat{O}\ket{\Phi}$$
and:
$$\hat{O}\,(\alpha\ket{\Psi}) = \alpha \,\hat{O}\,\ket{\Psi}$$

We associate expectation values with inner products:
$$\braket{O} \equiv \braket{\Psi|\hat{O}\Psi} = \int_{-\infty}^{+\infty}\; \Psi^*(x,t) \, \hat{O} \, \Psi(x,t) \, dx$$
For an operator $\hat{O}$ we define its hermitian adjoint $\hat{O}^\dagger$ by the behavior:
$$\braket{f|\hat{O}^\dagger g} = \braket{\hat{O} f| g}$$
or equivalently:
$$\braket{f|\hat{O} g} = \braket{\hat{O}^\dagger f| g}$$
When an operator is its own hermitian adjoint:
$$\hat{A}^\dagger = \hat{A}$$
we call it a hermitian operator, and it's expectation value is real:
$$\braket{A} = \braket{\Psi|\hat{A}\Psi} = \braket{\hat{A}^\dagger \Psi|\Psi}=\braket{\hat{A}\Psi|\Psi}=\braket{\Psi|\hat{A}\Psi}^* = \braket{A}^*$$
Therefore, we associate physical observables with hermitian operators.


\section{Determinate States}

We've accepted that Quantum Mechanics is indeterminate, in the sense
that if we prepare many copies of a system, each described by the same
state vector $\ket{\Psi}$, and we repeat the same measurement for each
copy, then the outcome of each of those measurements may be different.

Now we wish to consider if it is possible to prepare a system that is
gauranteed to give a particular result $q$ for the measurement of an observable $\hat{Q}$.  We already know of one example of such a state:  the stational solutions of the Schrodinger equations have definite energy, and we saw that they are Eigenenfunctions of the Hamiltonian operator.  We can write this with our updated notation as:
$$\hat{H} \ket{\psi} = E \ket{\psi}$$
which says equivalently that they are Eigenvectors of the Hamilitonian operator.  Let's see if this generalizes.

For a determinate state, every measurement results in $q$, so then:
$$\braket{\hat{Q}} = q$$
but also:
$$\sigma^2 = \braket{\left(\hat{Q}-\braket{Q}\right)^2} = 0$$
and so:
$$0 = \braket{\left(\hat{Q}-q\right)^2} = \braket{\Psi|\left(\hat{Q}-q\right)^2\Psi}$$
but since $\hat{Q}$ is an observable and $q$ is a real number $\hat{Q}-q$ is a hermitian operator so:

$$\braket{\Psi|\left(\hat{Q}-q\right)^2\Psi} = \braket{\left(\hat{Q}-q\right)\Psi|\left(\hat{Q}-q\right)\Psi} = 0$$
but an axiomatic property of the inner product is that:
$$\braket{f|f} = 0 \iff \ket{f} = 0$$
so we have:
\begin{eqnarray*}
 0 &=& \ket{\left(\hat{Q}-q\right)\Psi}\\
 0 &=& \left(\hat{Q}-q\right)\ket{\Psi}\\
\hat{Q}\ket{\Psi} &=& q \ket{\Psi}\\
\end{eqnarray*}
So, exactly as we found already for the specific case of the Hamiltonian operator, the eigenvectors of any observable (hermetian operator) $\hat{Q}$ are determinate states.
Note that the vector $0$ is never considered to be an eigenvector (as
it would trivially be an eigenvector of every operator, adding nothing
of value), but the value $0$ can certainly be an eigenvalue.  The collection of all the eigenvalues is called the spectrum of an operator.  

\section{Eigenfunctions of Operators with Discrete and Non-Degenerate Spectra}

Let's consider the case of an operator $\hat{Q}$ with a discrete (whether finite or infinite) spectrum of eigenvalues $\{\alpha_i\}$.  In this case, it is convenient to label the state vector of each determinate state by it's eigenvalue:
$$\hat{Q}\ket{\alpha_i} = \alpha_i \ket{\alpha_i}$$

If one vector is a scalar times another vector:
$$\ket{\alpha_i} = k \ket{\alpha_j}$$
we call them linearly dependent.  When this is not the case they are linearly independent.
Sometimes two linearly independent vectors $\ket{\alpha_i}$ and $\ket{\alpha_j}$ have the same eigenvalue for an operator $\hat{Q}$:
$$\alpha_i = \alpha_j$$
In this case, we call the spectrum degenerate.  Here we are assuming the spectrum is non-degenerate, and tackle the minor complexity of degeneracy later.

In the case an operator with discrete non-generate spectrum, the determinate states have some useful properties:

\begin{itemize} 
\item {\bf Their eigenvalues are real.}  This is much like we showed the expectation values of a hermitian operators are real.  Because $\hat{Q}$ is hermitian we have:
$$\braket{\alpha_i | \hat{Q} \alpha_i} = \braket{\hat{Q} \alpha_i | \alpha_i}$$
so:
\begin{eqnarray*}
q \braket{\alpha_i | \alpha_i} &=& q^* \braket{\alpha_i | \alpha_i}\\
(q-q^*) \braket{\alpha_i | \alpha_i} &=& 0\\
\end{eqnarray*}
But by definition eigenvectors cannot be zero, so:
$$\ket{\alpha_i} \neq 0 \implies \braket{\alpha_i | \alpha_i} \neq 0$$
So 
$$q=q^*$$
which shows that $q$ is real.
\item {\bf They are orthogonal if their eigenvalues are distinct:}  
\begin{eqnarray*}
\braket{\alpha_i | \hat{Q} \alpha_j} &=& \braket{\hat{Q} \alpha_i | \alpha_j}\\
\alpha_j \braket{\alpha_i | \alpha_j} &=& \alpha_i \braket{\alpha_i | \alpha_j}\\
(\alpha_j-\alpha_i) \braket{\alpha_i | \alpha_j} &=& 0\\
\end{eqnarray*}
So:
$$\alpha_j \neq \alpha_i \implies \braket{\alpha_i | \alpha_j} = 0$$

\item {\bf They are complete.}  Below we show this in the general case for a finite dimensional Hilbert space.  We've also already seen it hold true in infinite dimensions in the case of the Fourier series solutions to the infinite square well potential.  Beyond this, we will ``assume the eigenfunctions of an observable are complete unless proven otherwise.'' 

\end{itemize}

\section{Eigenfunctions of Hermitian Operators with Degenerate Spectra}

If one vector is a scalar times another vector:
$$\ket{\alpha_i} = k \ket{\alpha_j}$$
we call them linearly dependent.  When this is not the case they are linearly independent.
Sometimes two linearly independent vectors $\ket{\alpha_i}$ and $\ket{\alpha_j}$ have the same eigenvalue for an operator $\hat{Q}$:
$$\alpha_i = \alpha_j$$
and in this case we say the spectrum is degenerate\footnote{By the way, be careful not to take our notation too literally here, we still have:
$$\ket{\alpha_i} \neq \ket{\alpha_j}$$
because it is completely clear which vector is which, even though $\alpha_i=\alpha_j$!}
In this case, even though we cannot assume these eigenfunctions are orthogonal, we can arrange them so.  Consider the vector:
$$\ket{x} \equiv \ket{\alpha_j} - \braket{\alpha_i|\alpha_j}\ket{\alpha_i}$$
it is orthogonal to $\ket{\alpha_i}$ as we can see from:
$$\braket{\alpha_i| x} = \braket{\alpha_i | \alpha_j} - \braket{\alpha_i|\alpha_j} \braket{\alpha_i|\alpha_i} = 0$$
Furthermore, it cannot be zero for otherwise we would have:
$$\ket{\alpha_j} = \braket{\alpha_i|\alpha_j}\ket{\alpha_i}$$
and we already said $\ket{\alpha_j}$ and $\ket{\alpha_i}$ were linearly independent.
Lastly, it too has eigenvalue $\alpha_j$:
\begin{eqnarray*}
\hat{Q}\ket{x} &=& \hat{Q}\ket{\alpha_j} - \braket{\alpha_i|\alpha_j}\hat{Q}\ket{\alpha_i}\\
&=&\alpha_j \ket{\alpha_j} - \braket{\alpha_i|\alpha_j} \alpha_i \ket{\alpha_i}\\
&=& \alpha_j \ket{x}\\
\end{eqnarray*}
This means $\ket{x}$ has the same eigenvalue as $\ket{\alpha_j}$ but is also orthogonal to $\ket{\alpha_i}$.  So we need only normalize $\ket{x}$ and then redefine:
$$\ket{\alpha_j} \equiv \ket{x}$$
so that now:
$$\braket{\alpha_i | \alpha_j} = 0$$
even though:
$$\alpha_i = \alpha_j$$
We can extend this to any number of vectors with degenerate eigenvalues as needed to obtain an orthogonal set of eigenvectors.  This process is called the Gram-Schmidt orthogonalization. 

To summarize, for a hermitian operator $\hat{Q}$ with non-degenerate discrete spectrum $\{\alpha_i\}$ then the eigenfunctions, once properly normalized, are orthonormal:
$$\braket{\alpha_i | \alpha_j} = \delta_ij$$
For a degenerate discrete spectrum, we obtain the same result, but only by applying the Gram-Schmidt orthogonalization procedure first.

\section{The Dirac Delta Function}

It is both mathematically dubious and extremely useful to consider the Fourier Transform of the Dirac Delta function:
$$
\widetilde{\delta}(k) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \delta(x) e^{-ikx} dx  = \frac{e^{-ik0}}{\sqrt{2\pi}} = \frac{1}{\sqrt{2\pi}} 
$$
and so we can write the Dirac delta function as:
$$
\delta(x) = \frac{1}{2\pi} \int_{-\infty}^{\infty} e^{ikx} dk
$$
and so also:
$$
\delta(x-x') = \frac{1}{2\pi} \int_{-\infty}^{\infty} e^{ik(x-x')} dk
$$
and (by changing variables):
$$
\delta(k-k') = \frac{1}{2\pi} \int_{-\infty}^{\infty} e^{i(k-k')x)} dx
$$


\section{Eigenfunctions of Hermitian Operators with Continuous Spectra}

Let's consider the case of an operator $\hat{Q}$ with a discrete (whether finite or infinite) spectrum of eigenvalues $\{\alpha\}$.  In this case, it is convenient to label the state vector of each determinate state by it's eigenvalue:
$$\hat{Q}\ket{\alpha} = \alpha \ket{\alpha}$$
and 
$$\hat{Q}\ket{\beta} = \alpha \ket{\beta}$$

For the case of discrete non-generate spectrum, we saw that the eigenvalues were real, and the eigenvectors were orthogonal and complete.

In the case of continuous spectra, the reality follows exactly as it did in the discrete case.





\section{The Fourier Transform Revisited}

Our inner product now extends between positive and negative infinity:
\begin{equation}
\braket{\Psi, \phi} \equiv \int_{-\infty}^{\infty} \Psi^*(x) \phi(x) \, dx
\end{equation}
Our basis functions, which are now defined for any value of $k$,
\begin{equation}
e_k = \frac{1}{\sqrt{2\pi}} \exp(i k x)
\end{equation}
are still orthonormal, but the condition looks a bit different in the continuum case:
\begin{eqnarray*}
\braket{e_k, e_{k'}} &=& \delta(k-k')
\end{eqnarray*}
See the appendix for more details on the Dirac delta function $\delta(x)$, which is zero everywhere but at $x=0$, where it is infinite.  It is the continuous version of $\delta_{nm}$.

Our basis functions are also still complete.  In the discrete case we have a complex Fourier coefficient for every integer $n$.   Now we have a complex Fourier coefficient for any real value of $k$.  In place of Fourier coefficients, we have instead a function of $k$ which we call the Fourier transform: $\widetilde{\Psi}(k)$.
Instead of a sum over discrete terms, we now have to integrate over all values of $k$:
\begin{equation} \label{eqn:ift}
\Psi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \widetilde{\Psi}(k) \exp(ikx) \, dk.
\end{equation}
Just as in the discrete case, we determine the Fourier transform from the inner product:
\begin{equation} \label{eqn:ft}
\widetilde{\Psi}(k) = \braket{e_k, \Psi} = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} {\Psi}(x) \exp(-ikx) \, dx
\end{equation}
Equation~\ref{eqn:ft} is generally referred to as the {\em Fourier Transform}, while Equation~\ref{eqn:ift} is referred to as the {\em Inverse Fourier Transform}.

\section{The Fourier Transform in Quantum Mechanics}

So far we have been considering the Fourier transform with respect to position $x$ and wave-number $k$.  A much more useful pair of variables for Quantum Mechanics turns out to be momentum $p$ and position $x$.  To relate $p$ to $k$ we need only apply the DeBroglie relation to the wavelength in the definition of the wavenumber:
\begin{displaymath}
k \equiv \frac{2 \pi}{\lambda} = \frac{2 \pi p}{h} = \frac{p}{\hbar}
\end{displaymath}
We could therefore make the substitution $k \to p/\hbar$ (and $dk \to dp / \hbar)$) in Equations~\ref{eqn:ift} and ~\ref{eqn:ft}.  It turns out that a marginally more useful equation results if we make the normalization factors symmetric, by splitting the normalization factor of $1/\hbar$ across both equations with $1/\sqrt{\hbar}$ applied to each:
\begin{eqnarray} 
\Psi(x) &=& \frac{1}{\sqrt{2\pi\hbar}} \int_{-\infty}^{\infty} \widetilde{\Psi}(p) \exp(ipx/\hbar) \, dp \\
\widetilde{\Psi}(p) &=&  \frac{1}{\sqrt{2\pi\hbar}} \int_{-\infty}^{\infty} {\Psi}(x) \exp(-ipx/\hbar) \, dx
\end{eqnarray}
The major benefit of this symmetric form is that the normalization of $\Psi(x)$ and $\widetilde{\Psi}(p)$ in this case turns out to be the same:
\begin{displaymath}
\int_{-\infty}^{\infty} |\Psi(x)|^2 dx = \int_{-\infty}^{\infty} |\widetilde{\Psi}(p)|^2 dp = 1 
\end{displaymath}
Because we can always calculate $\Psi(x)$ from $\widetilde{\Psi}(p)$ either one completely describes the quantum mechanical state.  We call $\widetilde{\Psi}(p)$ the momentum wave function.   Whereas $|\Psi(x)|^2$ gives us the probability density for the quanton to be at position $x$, $|\Psi(p)|^2$ gives us the probability density for the quanton to have momentum $p$.
$$\int_{-\\infty}^{+\infty} |f(x)|^2 dx < \infty$$


\section{Completeness of Eigenvectors of a Hermitian Operator}

Here we will show that the eigenvectors of a Hermitian operator $Q$ are complete in any $n$ dimensional Hilbert space.

It is evidently true for $n=1$.  We will assume it is true for $n-1$ and show that it is true for $n$.  The proof will follow from induction.

Take any eigenvector of $\hat{Q}$ in an $n$-dimensional Hilbert space  $V$.
$$\hat{Q}\ket{\lambda_0} = \lambda_0 \ket{\lambda_0}$$
Now consider the subspace $S^\perp$ of all vectors perpendicular to $\ket{\lambda}$:
$$S^\perp \equiv \{ \ket{x} \; {\rm such \; that} \; \braket{x|\lambda_0}=0 \}$$

The subspace $S^\perp$ has dimension $n-1$ by construction.  To see this, note that $S^\perp$ has dimension $n$ at most, so we can write any $\ket{x}$ in $S^\perp$ in terms of $n$ orthonormal basis vectors:
$$\ket{x} = A_0 \ket{\lambda_0} + A_1 \ket{x_1} + A_2 \ket{x_2} \ldots A_{n-1} \ket{x_{n-1}}$$
but
$$A_0 = \braket{\lambda_0|x} = 0$$
so really:
$$\ket{x} = A_1 \ket{x_1} + A_2 \ket{x_2} \ldots A_{n-1} \ket{x_{n-1}}$$
and we see that $S^\perp$ has $n-1$ dimensions as intended.

Next we'll show that the $n-1$ dimensional subspace $S^\perp$ is closed under $\hat{Q}$, so that 
we can consider $\hat{Q}$ as a Hermitian operator in an $n-1$ dimensional space, and the induction hypothesis applies.  For $\ket{x} \in S^\perp$ we have:
$$\braket{Qx|\lambda} = \braket{x| Q\lambda} = \lambda \braket{x|\lambda} = 0$$
which shows that $Q\ket{x}$ is in $S^\perp$ as well.  

That means $n-1$ eigenvectors of $\hat{Q}$ span the $n-1$ dimensional space by our induction hypothesis.  

As the $n-1$ dimensional subspace $S^\perp$ is closed under $\hat{Q}$ 


\end{document}




